{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d3e45b",
   "metadata": {},
   "source": [
    "### Invocation\n",
    "- Invoke\n",
    "- Stream\n",
    "    - `stream()`, `astream_events()`\n",
    "- Batch\n",
    "    - `batch()`, `batch_as_completed()`\n",
    "- Tool Call\n",
    "- Structured Output\n",
    "- Multimodal\n",
    "- Reasoning\n",
    "- Caching\n",
    "- Rate limiting\n",
    "- Base URL or proxy\n",
    "- Log probabilities\n",
    "- Token usage\n",
    "- Invocation config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834f9ad",
   "metadata": {},
   "source": [
    "### 기본 사용법\n",
    "채팅 모델\n",
    "`init_chat_model` 로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ce443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai:gpt-5-nano\")\n",
    "response = model.invoke(\"Why do parrots talk?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444ff48",
   "metadata": {},
   "source": [
    "### Chat Model Parameters\n",
    "\n",
    "Chat model의 동작을 설정하는 데 사용되는 파라미터들입니다. 지원되는 파라미터는 모델 및 제공자에 따라 다를 수 있습니다.\n",
    "\n",
    "**`model`**\n",
    "-   **Type**: `string`\n",
    "-   **Required**: Yes\n",
    "-   **Description**: 사용하려는 특정 모델의 이름이나 식별자입니다.\n",
    "\n",
    "**`api_key`**\n",
    "-   **Type**: `string`\n",
    "-   **Required**: No\n",
    "-   **Description**: 모델 제공자에 대한 인증에 필요한 키입니다. 보통 모델 접근을 위해 가입할 때 발급되며, 환경 변수로 설정하여 접근하는 경우가 많습니다.\n",
    "\n",
    "**`temperature`**\n",
    "-   **Type**: `number`\n",
    "-   **Required**: No\n",
    "-   **Description**: 모델 출력의 무작위성을 제어합니다. 값이 높을수록 응답이 더 창의적이 되고, 낮을수록 더 결정론적인 응답이 나옵니다.\n",
    "\n",
    "**`stop`**\n",
    "-   **Type**: `string[]`\n",
    "-   **Required**: No\n",
    "-   **Description**: 모델이 출력 생성을 중단해야 할 시점을 나타내는 문자 시퀀스입니다.\n",
    "\n",
    "**`timeout`**\n",
    "-   **Type**: `number`\n",
    "-   **Required**: No\n",
    "-   **Description**: 요청을 취소하기 전, 모델로부터 응답을 기다리는 최대 시간(초)입니다.\n",
    "\n",
    "**`max_tokens`**\n",
    "-   **Type**: `number`\n",
    "-   **Required**: No\n",
    "-   **Description**: 응답의 총 토큰 수를 제한하여 출력 길이를 효과적으로 제어합니다.\n",
    "\n",
    "**`max_retries`**\n",
    "-   **Type**: `number`\n",
    "-   **Required**: No\n",
    "-   **Description**: 네트워크 타임아웃이나 속도 제한과 같은 문제로 요청이 실패할 경우, 시스템이 요청을 다시 보내는 최대 시도 횟수입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8dbdd3",
   "metadata": {},
   "source": [
    "### Invoke\n",
    "모델을 호출 하는 가장 간단한 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5412c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\"Why do parrots have colorful feathers?\")\n",
    "print(response)\n",
    "\n",
    "# 대화 내역을 나타내기 위해 메시지 목록을 제공.\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "conversation = [\n",
    "    SystemMessage(\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(\"Translate: I love programming.\"),\n",
    "    AIMessage(\"J'adore la programmation.\"),\n",
    "    HumanMessage(\"Translate: I love building applications.\")\n",
    "]\n",
    "\n",
    "response = model.invoke(conversation)\n",
    "print(response)  # AIMessage(\"J'adore créer des applications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d6581e",
   "metadata": {},
   "source": [
    "### Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)\n",
    "\n",
    "# 모델이 전체 응답을 완료한 후, 단일 객체`AIMessage`를 반환하는 invoke()와 달리, \n",
    "# stream()은 출력 테스트의 일부를 포함하는 여러 객체`AIMessageChunk`를 반환.\n",
    "full = None  # None | AIMessageChunk\n",
    "for chunk in model.stream(\"What color is the sky?\"):\n",
    "    full = chunk if full is None else full + chunk\n",
    "    print(full.text)\n",
    "\n",
    "# The\n",
    "# The sky\n",
    "# The sky is\n",
    "# The sky is typically\n",
    "# The sky is typically blue\n",
    "# ...\n",
    "\n",
    "print(full.content_blocks)\n",
    "# [{\"type\": \"text\", \"text\": \"The sky is typically blue...\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f13dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# astream_events()\n",
    "# 이벤트 유형 및 기타 메타데이터를 기반으로 필터링이 간소화되고,\n",
    "# 백그라운드에서 전체 메시지가 집계\n",
    "async for event in model.astream_events(\"Hello\"):\n",
    "\n",
    "    if event[\"event\"] == \"on_chat_model_start\":\n",
    "        print(f\"Input: {event['data']['input']}\")\n",
    "\n",
    "    elif event[\"event\"] == \"on_chat_model_stream\":\n",
    "        print(f\"Token: {event['data']['chunk'].text}\")\n",
    "\n",
    "    elif event[\"event\"] == \"on_chat_model_end\":\n",
    "        print(f\"Full message: {event['data']['output'].text}\")\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "# Input: Hello\n",
    "# Token: Hi\n",
    "# Token:  there\n",
    "# Token: !\n",
    "# Token:  How\n",
    "# Token:  can\n",
    "# Token:  I\n",
    "# ...\n",
    "# Full message: Hi there! How can I help today?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bfd9ca",
   "metadata": {},
   "source": [
    "### Batch\n",
    "모델에 대한 요청을 병렬로 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = model.batch([\n",
    "    \"Why do parrots have colorful feathers?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is quantum computing?\"\n",
    "])\n",
    "for response in responses:\n",
    "    print(response)\n",
    "# 기본적으로 전체 배치에 대한 최종 출력만 반환.\n",
    "# 생성이 완료될 때마다 각 개별 입력에 대한 출력을 받으려면 \n",
    "# `batch_as_completed()` 사용\n",
    "for response in model.batch_as_completed([\n",
    "    \"Why do parrots have colorful feathers?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is quantum computing?\"\n",
    "]):\n",
    "    print(response)\n",
    "\n",
    "# batch(), 또는 batch_as_completed() 사용시 `max_concurrency` 속성으로 최대 병렬 호출 수를 제어\n",
    "# model.batch(\n",
    "#     list_of_inputs,\n",
    "#     config={\n",
    "#         'max_concurrency': 5,  # Limit to 5 parallel calls\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63de7c0f",
   "metadata": {},
   "source": [
    "### Tool Call\n",
    "`bind_tools()` 이후 호출 시 모델은 필요에 따라 바인딩된 도구를 호출할 수 있음.\n",
    "- 일반 호출\n",
    "- 도구 호출 루프\n",
    "- 도구 호출 강제\n",
    "- 병렬 도구 호출\n",
    "- 스트리밍 도구 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the weather at a location.\"\"\"\n",
    "    return f\"It's sunny in {location}.\"\n",
    "\n",
    "\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "response = model_with_tools.invoke(\"What's the weather like in Boston?\")\n",
    "for tool_call in response.tool_calls:\n",
    "    # View tool calls made by the model\n",
    "    print(f\"Tool: {tool_call['name']}\")\n",
    "    print(f\"Args: {tool_call['args']}\")\n",
    "\n",
    "# ---\n",
    "# * 도구 호출 루프\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "# Step 1: Model generates tool calls\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\n",
    "ai_msg = model_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "\n",
    "# Step 2: Execute tools and collect results\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    # Execute the tool with the generated arguments\n",
    "    tool_result = get_weather.invoke(tool_call)\n",
    "    messages.append(tool_result)\n",
    "\n",
    "# Step 3: Pass results back to model for final response\n",
    "final_response = model_with_tools.invoke(messages)\n",
    "print(final_response.text)\n",
    "# \"The current weather in Boston is 72°F and sunny.\"\n",
    "\n",
    "# ---\n",
    "# * 도구 호출 강제\n",
    "model_with_tools = model.bind_tools([tool], tool_choice=\"any\")\n",
    "\n",
    "# ---\n",
    "# * 병렬 도구 호출\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "response = model_with_tools.invoke(\n",
    "    \"What's the weather in Boston and Tokyo?\"\n",
    ")\n",
    "\n",
    "# The model may generate multiple tool calls\n",
    "print(response.tool_calls)\n",
    "# [\n",
    "#   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},\n",
    "#   {'name': 'get_time', 'args': {'location': 'Tokyo'}, 'id': 'call_2'}\n",
    "# ]\n",
    "\n",
    "\n",
    "# Execute all tools (can be done in parallel with async)\n",
    "results = []\n",
    "for tool_call in response.tool_calls:\n",
    "    if tool_call['name'] == 'get_weather':\n",
    "        result = get_weather.invoke(tool_call)\n",
    "    ...\n",
    "    results.append(result)\n",
    "\n",
    "# ---\n",
    "# * 스트리밍 도구 호출\n",
    "for chunk in model_with_tools.stream(\n",
    "    \"What's the weather in Boston and Tokyo?\"\n",
    "):\n",
    "    # Tool call chunks arrive progressively\n",
    "    if chunk.tool_call_chunks:\n",
    "        for tool_chunk in chunk.tool_call_chunks:\n",
    "            print(f\"Tool: {tool_chunk.get('name', '')}\")\n",
    "            print(f\"Args: {tool_chunk.get('args', '')}\")\n",
    "\n",
    "# Output:\n",
    "# Tool: get_weather            # Loop 1\n",
    "# Args:\n",
    "# Tool:                        # Loop 2\n",
    "# Args: {\"loc\n",
    "# Tool:                        # Loop 3\n",
    "# Args: ation\": \"BOS\"}\n",
    "# Tool: get_time               # Loop 4\n",
    "# Args:\n",
    "# Args:\n",
    "# Tool:                        # Loop 5\n",
    "# Args: {\"timezone\": \"Tokyo\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa9295e",
   "metadata": {},
   "source": [
    "### 구조화된 출력 (Structured Output)\n",
    "모델은 주어진 스키마와 일치하는 형식으로 응답을 제공하도록 요청할 수 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef2dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic 모델은 필드 검증, 설명 및 중첩 구조를 갖춘 가장 풍부한 기능 세트를 제공\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "model_with_structure = model.with_structured_output(Movie)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)\n",
    "\n",
    "# ---\n",
    "# 구문 분석된 구조와 함께 메시지 출력\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "# 원시 객체 포함: include_raw=True \n",
    "model_with_structure = model.with_structured_output(Movie, include_raw=True)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "response\n",
    "# {\n",
    "#     \"raw\": AIMessage(...),\n",
    "#     \"parsed\": Movie(title=..., year=..., ...),\n",
    "#     \"parsing_error\": None,\n",
    "# }\n",
    "# AIMessage와 같은 응답 메타데이터에 엑세스하귀 위해 구문 분석된 표현과 함께 원시 객체를 반환하는 것이 유요할 수도 있음.\n",
    "\n",
    "# ---\n",
    "# 중첩 구조\n",
    "class Actor(BaseModel):\n",
    "    name: str\n",
    "    role: str\n",
    "\n",
    "class MovieDetails(BaseModel):\n",
    "    title: str\n",
    "    year: int\n",
    "    cast: list[Actor]\n",
    "    genres: list[str]\n",
    "    budget: float | None = Field(None, description=\"Budget in millions USD\")\n",
    "\n",
    "model_with_structure = model.with_structured_output(MovieDetails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de7483",
   "metadata": {},
   "source": [
    "### Multimodal\n",
    "특정 모델은 이미지, 오디오, 비디오와 같은 비텍스트 데이터를 처리하고 반환할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7753216",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\"Create a picture of a cat\")\n",
    "print(response.content_blocks)\n",
    "# [\n",
    "#     {\"type\": \"text\", \"text\": \"Here's a picture of a cat\"},\n",
    "#     {\"type\": \"image\", \"base64\": \"...\", \"mime_type\": \"image/jpeg\"},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef1bae",
   "metadata": {},
   "source": [
    "### Reasoning\n",
    "최신 모델은 결론에 도달하기 위해 다단계 추론을 수행할 수 있음. 기본 모델이 뒷받침하는 경우, 이 추론 과정을 표현화하여 모델이 최종 응답에 도달한 과정을 더 잘 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d17b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream\n",
    "for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n",
    "    reasoning_steps = [r for r in chunk.content_blocks if r[\"type\"] == \"reasoning\"]\n",
    "    print(reasoning_steps if reasoning_steps else chunk.text)\n",
    "\n",
    "# invoke\n",
    "response = model.invoke(\"Why do parrots have colorful feathers?\")\n",
    "reasoning_steps = [b for b in response.content_blocks if b[\"type\"] == \"reasoning\"]\n",
    "print(\" \".join(step[\"reasoning\"] for step in reasoning_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aab429",
   "metadata": {},
   "source": [
    "### 캐싱 (Caching)\n",
    "채팅 모델은 API 호출 속도가 느리고 비용이 많이 들 수 있음. 이를 완화하기 위해 LangChain은 채팅 모델 통합을 위한 캐싱 계층을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa35e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적으로 캐싱은 비활성화되어 있음. 활성화하려면 다음을 사용.\n",
    "from langchain_core.globals import set_llm_cache\n",
    "\n",
    "# ---\n",
    "# 메모리 캐시\n",
    "from langchain_core.caches import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "response = model.invoke(\"Tell me a joke\")\n",
    "response = model.invoke(\"Tell me a joke\")  # Fast, from cache\n",
    "\n",
    "# ---\n",
    "# SQLite 캐시\n",
    "from langchain_community.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "response = model.invoke(\"Tell me a joke\")\n",
    "response = model.invoke(\"Tell me a joke\")  # Fast, from cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b3f21",
   "metadata": {},
   "source": [
    "### 속도 제한 (Rate limiting)\n",
    "많은 채팅 모델 제공업체는 특정 기간 동안 수행할 수 있는 호출 횟수에 제한을 둠. 속도 제한에 도달하면 일반적으로 제공업체로부터 속도 제한 오류 응답을 받게 되며, 추가 요청을 하려면 기다려야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443fe1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "# 단위 시간당 요청 수만 제한 가능. 요청 크기에 따라 제한을 적용해야 하는 경우에는 도움이 되지 않음.\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "        requests_per_second=0.1,  # 1 request every 10s\n",
    "        check_every_n_seconds=0.1,  # Check every 100ms whether allowed to make a request\n",
    "        max_bucket_size=10,  # Controls the maximum burst size.\n",
    ")\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-5\",\n",
    "    model_provider=\"openai\",\n",
    "    rate_limiter=rate_limiter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56761a2",
   "metadata": {},
   "source": [
    "### Base URL or proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5becba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI\n",
    "model = init_chat_model(\n",
    "    model=\"MODEL_NAME\",\n",
    "    model_provider=\"openai\",\n",
    "    base_url=\"BASE_URL\",\n",
    "    api_key=\"YOUR_API_KEY\",\n",
    ")\n",
    "\n",
    "# Http Proxy\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    openai_proxy=\"http://proxy.example.com:8080\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae0a80",
   "metadata": {},
   "source": [
    "### Log probabilities\n",
    "특정 모델은 주어진 토큰의 발생 가능성을 나타내는 토큰 수준의 로그 확률을 반환하도록 구성할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    model=\"gpt-4o\",\n",
    "    model_provider=\"openai\"\n",
    ").bind(logprobs=True)\n",
    "\n",
    "response = model.invoke(\"Why do parrots talk?\")\n",
    "print(response.response_metadata[\"logprobs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a8ef41",
   "metadata": {},
   "source": [
    "### 토큰 사용 (Token usage)\n",
    "여러 모델 제공자가 호출 응답의 일부로 토큰 사용 정보를 반환합니다. 이 정보가 제공되는 경우, 해당 모델에서 생성된 AIMessage 객체에 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "\n",
    "llm_1 = init_chat_model(model=\"openai:gpt-4o-mini\")\n",
    "llm_2 = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n",
    "\n",
    "callback = UsageMetadataCallbackHandler()\n",
    "result_1 = llm_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
    "result_2 = llm_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
    "callback.usage_metadata\n",
    "# {\n",
    "#     'gpt-4o-mini-2024-07-18': {\n",
    "#         'input_tokens': 8,\n",
    "#         'output_tokens': 10,\n",
    "#         'total_tokens': 18,\n",
    "#         'input_token_details': {'audio': 0, 'cache_read': 0},\n",
    "#         'output_token_details': {'audio': 0, 'reasoning': 0}},\n",
    "#         'claude-3-5-haiku-20241022': {'input_tokens': 8,\n",
    "#         'output_tokens': 21,\n",
    "#         'total_tokens': 29,\n",
    "#         'input_token_details': {'cache_read': 0, 'cache_creation': 0}\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8595ba15",
   "metadata": {},
   "source": [
    "### Invocation config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_callback_handler():\n",
    "    ...\n",
    "response = model.invoke(\n",
    "    \"Tell me a joke\",\n",
    "        config={\n",
    "        \"run_name\": \"joke_generation\",      # Custom name for this run\n",
    "        \"tags\": [\"humor\", \"demo\"],          # Tags for categorization\n",
    "        \"metadata\": {\"user_id\": \"123\"},     # Custom metadata\n",
    "        \"callbacks\": [my_callback_handler], # Callback handlers\n",
    "    }\n",
    ")\n",
    "\n",
    "# ---\n",
    "\n",
    "# configurable_fields 를 지정하여 런타임 구성 가능 모델을 생성할 수도 있음.\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "configurable_model = init_chat_model(temperature=0)\n",
    "\n",
    "configurable_model.invoke(\n",
    "    \"what's your name\",\n",
    "        config={\"configurable\": {\"model\": \"gpt-5-nano\"}},  # Run with GPT-5-Nano\n",
    ")\n",
    "configurable_model.invoke(\n",
    "    \"what's your name\",\n",
    "        config={\"configurable\": {\"model\": \"claude-3-5-sonnet-latest\"}},  # Run with Claude\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

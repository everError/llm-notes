{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce1eeef",
   "metadata": {},
   "source": [
    "- í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from typing import TypedDict, List, Annotated\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import operator\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ìƒíƒœ ì •ì˜\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List, operator.add]  # ë©”ì‹œì§€ ëˆ„ì \n",
    "    phase: str  # í˜„ì¬ ë‹¨ê³„: \"thinking\", \"finalizing\"\n",
    "    iteration: int  # ë£¨í”„ íšŸìˆ˜ ì¶”ì \n",
    "\n",
    "# ë„êµ¬ ì •ì˜\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the given city`s weather\"\"\"\n",
    "    return f\"{city}ì˜ ë‚ ì”¨ëŠ” ë§‘ì•„ìš”!\"\n",
    "\n",
    "# LLM ì„¤ì • (ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”)\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\", streaming=True)\n",
    "tools = [get_weather]\n",
    "\n",
    "# ReAct í”„ë¡¬í”„íŠ¸: ë„êµ¬ í˜¸ì¶œê³¼ ì¶”ë¡  ë£¨í”„ ì§€ì›\n",
    "react_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful assistant. Respond in ReAct format:\n",
    "1. Always provide: Thought: [Your step-by-step reasoning]\n",
    "2. If a tool is needed, use the tool-calling mechanism to invoke it\n",
    "3. If no tool is needed but reasoning is incomplete, output: Continue: [next reasoning step or sub-question]\n",
    "4. If reasoning is complete and no tool is needed, output: Final Answer: [final response]\n",
    "Every response MUST include a Thought in the content field, even when using tool-calling.\n",
    "Do NOT use tools unless the query explicitly requires external data.\n",
    "For reasoning tasks, use Continue for intermediate steps.\"\"\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "# LLM ë°”ì¸ë“œ (ë„êµ¬ ë°”ì¸ë”©)\n",
    "agent_runnable = react_prompt | llm.bind_tools(tools)\n",
    "\n",
    "# ì—ì´ì „íŠ¸ ë…¸ë“œ\n",
    "def agent_node(state: AgentState) -> AgentState:\n",
    "    messages = state[\"messages\"]\n",
    "    phase = state.get(\"phase\", \"thinking\")\n",
    "    iteration = state.get(\"iteration\", 0) + 1\n",
    "    \n",
    "    # ë¬´í•œ ë£¨í”„ ë°©ì§€\n",
    "    if iteration > 10:\n",
    "        return {\n",
    "            \"messages\": messages + [AIMessage(content=\"Thought: Maximum iteration limit reached.\\nFinal Answer: Unable to complete reasoning.\")],\n",
    "            \"phase\": \"finalizing\",\n",
    "            \"iteration\": iteration\n",
    "        }\n",
    "    \n",
    "    result = agent_runnable.invoke({\"messages\": messages})\n",
    "    # ë¶€ì ì ˆí•œ tool_calls í•„í„°ë§ (ë¬¸ë§¥ê³¼ ë¬´ê´€í•œ ê²½ìš°)\n",
    "    # if result.tool_calls:\n",
    "    #     query = messages[0].content.lower()\n",
    "    #     valid_tool = any(tool[\"name\"] == \"get_weather\" and \"ë‚ ì”¨\" in query for tool in result.tool_calls)\n",
    "    #     if not valid_tool:\n",
    "    #         result.tool_calls = []  # ë¶€ì ì ˆí•œ ë„êµ¬ í˜¸ì¶œ ì œê±°\n",
    "    #         result.content = f\"{result.content}\\nContinue: Re-evaluate the query without tools.\"\n",
    "    \n",
    "    new_phase = \"finalizing\" if result.tool_calls or \"Final Answer\" in result.content else \"thinking\"\n",
    "    return {\"messages\": [result], \"phase\": new_phase, \"iteration\": iteration}\n",
    "\n",
    "# ë„êµ¬ ë…¸ë“œ\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# ê·¸ë˜í”„ ì •ì˜\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€\n",
    "def route_agent(state: AgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    content = last_message.content\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    if \"Final Answer\" in content:\n",
    "        return END\n",
    "    return \"agent\"  # Continue ë˜ëŠ” Thoughtë§Œ ìˆìœ¼ë©´ ë£¨í”„ë°±\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    route_agent,\n",
    "    {\"agent\": \"agent\", \"tools\": \"tools\", END: END}\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph = workflow.compile()\n",
    "\n",
    "# ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜\n",
    "async def stream_react(user_input: str):\n",
    "    try:\n",
    "        events = graph.astream_events(\n",
    "            {\"messages\": [HumanMessage(content=user_input)], \"phase\": \"thinking\", \"iteration\": 0},\n",
    "            version=\"v1\"\n",
    "        )\n",
    "        async for event in events:\n",
    "            kind = event[\"event\"]\n",
    "            if kind == \"on_llm_stream\":\n",
    "                chunk = event[\"data\"][\"chunk\"]\n",
    "                if chunk.content:\n",
    "                    yield f\"ğŸ¤” Thought (streaming): {chunk.content}\"\n",
    "            elif kind == \"on_chain_stream\":\n",
    "                if \"agent\" in event[\"tags\"]:\n",
    "                    if \"messages\" in event[\"data\"][\"chunk\"]:\n",
    "                        msg = event[\"data\"][\"chunk\"][\"messages\"][0]\n",
    "                        content = msg.content\n",
    "                        if content:\n",
    "                            yield f\"ğŸ¤” Full Thought: {content}\"\n",
    "                        if msg.tool_calls:\n",
    "                            for call in msg.tool_calls:\n",
    "                                yield f\"ğŸ”§ Action: {call['name']}({call['args']})\"\n",
    "                elif \"tools\" in event[\"tags\"]:\n",
    "                    if \"messages\" in event[\"data\"][\"chunk\"]:\n",
    "                        for msg in event[\"data\"][\"chunk\"][\"messages\"]:\n",
    "                            yield f\"ğŸ“Š Observation: {msg.content}\"\n",
    "            elif kind == \"on_chain_end\" and \"agent\" in event[\"tags\"]:\n",
    "                final_msg = event[\"data\"][\"output\"][\"messages\"][-1]\n",
    "                content = final_msg.content\n",
    "                if content and \"Final Answer\" in content:\n",
    "                    yield f\"âœ… Final Answer: {content.split('Final Answer:')[-1].strip()}\"\n",
    "    except Exception as e:\n",
    "        yield f\"âŒ Error: {str(e)}\"\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© main\n",
    "async def main():\n",
    "    # user_query = \"í•œ ë‹¬ê°„ì˜ ìœ ëŸ½ ë°°ë‚­ì—¬í–‰ ê³„íšì„ ì§œì¤˜. ì£¼ìš” ë„ì‹œëŠ” íŒŒë¦¬, ë¡œë§ˆ, ë² ë¥¼ë¦°ì´ê³ , ì˜ˆì‚°ì€ 500ë§Œ ì›ì´ì•¼.\"\n",
    "    user_query = \"ëŒ€êµ¬ ë‚ ì”¨ ì•Œë ¤ì¤˜.\"\n",
    "    async for step in stream_react(user_query):\n",
    "        print(step)\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce1eeef",
   "metadata": {},
   "source": [
    "- 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from typing import TypedDict, List, Annotated\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import operator\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY가 .env 파일에 설정되지 않았습니다.\")\n",
    "\n",
    "# 상태 정의\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List, operator.add]  # 메시지 누적\n",
    "    phase: str  # 현재 단계: \"thinking\", \"finalizing\"\n",
    "    iteration: int  # 루프 횟수 추적\n",
    "\n",
    "# 도구 정의\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the given city`s weather\"\"\"\n",
    "    return f\"{city}의 날씨는 맑아요!\"\n",
    "\n",
    "# LLM 설정 (스트리밍 활성화)\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\", streaming=True)\n",
    "tools = [get_weather]\n",
    "\n",
    "# ReAct 프롬프트: 도구 호출과 추론 루프 지원\n",
    "react_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful assistant. Respond in ReAct format:\n",
    "1. Always provide: Thought: [Your step-by-step reasoning]\n",
    "2. If a tool is needed, use the tool-calling mechanism to invoke it\n",
    "3. If no tool is needed but reasoning is incomplete, output: Continue: [next reasoning step or sub-question]\n",
    "4. If reasoning is complete and no tool is needed, output: Final Answer: [final response]\n",
    "Every response MUST include a Thought in the content field, even when using tool-calling.\n",
    "Do NOT use tools unless the query explicitly requires external data.\n",
    "For reasoning tasks, use Continue for intermediate steps.\"\"\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "# LLM 바인드 (도구 바인딩)\n",
    "agent_runnable = react_prompt | llm.bind_tools(tools)\n",
    "\n",
    "# 에이전트 노드\n",
    "def agent_node(state: AgentState) -> AgentState:\n",
    "    messages = state[\"messages\"]\n",
    "    phase = state.get(\"phase\", \"thinking\")\n",
    "    iteration = state.get(\"iteration\", 0) + 1\n",
    "    \n",
    "    # 무한 루프 방지\n",
    "    if iteration > 10:\n",
    "        return {\n",
    "            \"messages\": messages + [AIMessage(content=\"Thought: Maximum iteration limit reached.\\nFinal Answer: Unable to complete reasoning.\")],\n",
    "            \"phase\": \"finalizing\",\n",
    "            \"iteration\": iteration\n",
    "        }\n",
    "    \n",
    "    result = agent_runnable.invoke({\"messages\": messages})\n",
    "    # 부적절한 tool_calls 필터링 (문맥과 무관한 경우)\n",
    "    # if result.tool_calls:\n",
    "    #     query = messages[0].content.lower()\n",
    "    #     valid_tool = any(tool[\"name\"] == \"get_weather\" and \"날씨\" in query for tool in result.tool_calls)\n",
    "    #     if not valid_tool:\n",
    "    #         result.tool_calls = []  # 부적절한 도구 호출 제거\n",
    "    #         result.content = f\"{result.content}\\nContinue: Re-evaluate the query without tools.\"\n",
    "    \n",
    "    new_phase = \"finalizing\" if result.tool_calls or \"Final Answer\" in result.content else \"thinking\"\n",
    "    return {\"messages\": [result], \"phase\": new_phase, \"iteration\": iteration}\n",
    "\n",
    "# 도구 노드\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# 그래프 정의\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# 조건부 엣지\n",
    "def route_agent(state: AgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    content = last_message.content\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    if \"Final Answer\" in content:\n",
    "        return END\n",
    "    return \"agent\"  # Continue 또는 Thought만 있으면 루프백\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    route_agent,\n",
    "    {\"agent\": \"agent\", \"tools\": \"tools\", END: END}\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph = workflow.compile()\n",
    "\n",
    "# 비동기 스트리밍 함수\n",
    "async def stream_react(user_input: str):\n",
    "    try:\n",
    "        events = graph.astream_events(\n",
    "            {\"messages\": [HumanMessage(content=user_input)], \"phase\": \"thinking\", \"iteration\": 0},\n",
    "            version=\"v1\"\n",
    "        )\n",
    "        async for event in events:\n",
    "            kind = event[\"event\"]\n",
    "            if kind == \"on_llm_stream\":\n",
    "                chunk = event[\"data\"][\"chunk\"]\n",
    "                if chunk.content:\n",
    "                    yield f\"🤔 Thought (streaming): {chunk.content}\"\n",
    "            elif kind == \"on_chain_stream\":\n",
    "                if \"agent\" in event[\"tags\"]:\n",
    "                    if \"messages\" in event[\"data\"][\"chunk\"]:\n",
    "                        msg = event[\"data\"][\"chunk\"][\"messages\"][0]\n",
    "                        content = msg.content\n",
    "                        if content:\n",
    "                            yield f\"🤔 Full Thought: {content}\"\n",
    "                        if msg.tool_calls:\n",
    "                            for call in msg.tool_calls:\n",
    "                                yield f\"🔧 Action: {call['name']}({call['args']})\"\n",
    "                elif \"tools\" in event[\"tags\"]:\n",
    "                    if \"messages\" in event[\"data\"][\"chunk\"]:\n",
    "                        for msg in event[\"data\"][\"chunk\"][\"messages\"]:\n",
    "                            yield f\"📊 Observation: {msg.content}\"\n",
    "            elif kind == \"on_chain_end\" and \"agent\" in event[\"tags\"]:\n",
    "                final_msg = event[\"data\"][\"output\"][\"messages\"][-1]\n",
    "                content = final_msg.content\n",
    "                if content and \"Final Answer\" in content:\n",
    "                    yield f\"✅ Final Answer: {content.split('Final Answer:')[-1].strip()}\"\n",
    "    except Exception as e:\n",
    "        yield f\"❌ Error: {str(e)}\"\n",
    "\n",
    "# 테스트용 main\n",
    "async def main():\n",
    "    # user_query = \"한 달간의 유럽 배낭여행 계획을 짜줘. 주요 도시는 파리, 로마, 베를린이고, 예산은 500만 원이야.\"\n",
    "    user_query = \"대구 날씨 알려줘.\"\n",
    "    async for step in stream_react(user_query):\n",
    "        print(step)\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

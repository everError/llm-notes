{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39ac6b29",
   "metadata": {},
   "source": [
    "### Short-term Memory\n",
    "\n",
    "메모리는 이전 상호작용에 대한 정보를 기억하는 시스템입니다. AI 에이전트에게 메모리는 이전 상호작용을 기억하고, 피드백을 통해 학습하며, 사용자 선호도에 맞춰 정응할 수 있도록 해주기 때문에 매우 중요합니다. \n",
    "단기 메모리를 사용하면 애플리케이션이 단일 스레드 또는 대화 내용에 수행된 이전 상호 작용을 기억할 수 있습니다.\n",
    "\n",
    "대화 이력은 가장 흔한 형태의 단기 기억입니다. 긴 대화는 오늘날의 LLM(학습 목표)에 어려움을 야기합니다. 전체 이력이 LLM의 맥락 창에 맞지 ​​않아 맥락 손실이나 오류가 발생할 수 있습니다.\n",
    "모델이 전체 맥락 길이를 지원하더라도 대부분의 LLM은 긴 맥락에서는 여전히 성능이 좋지 않습니다. 오래되거나 주제에서 벗어난 콘텐츠로 인해 주의가 산만해지고, 응답 시간이 느려지고 비용이 증가하기 때문입니다.\n",
    "\n",
    "채팅 모델은 메시지를 사용하여 컨텍스트를 수용하는데 , 여기에는 지시(시스템 메시지)와 입력(사용자 메시지)이 포함됩니다. 채팅 애플리케이션에서 메시지는 사용자 입력과 모델 응답을 번갈아 가며 전달되므로 시간이 지남에 따라 메시지 목록이 길어집니다. 컨텍스트 창은 제한되어 있기 때문에 많은 애플리케이션에서 오래된 정보를 제거하거나 \"잊는\" 기술을 사용하면 이점을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0be4b4",
   "metadata": {},
   "source": [
    "- Usage\n",
    "에이전트에 단기 메모리(스레드 수준 지속성)를 추가하려면 에이전트를 생성할 때, `checkpointer` 를 지정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25d7b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='A 라는 유저는 몇살이야?', additional_kwargs={}, response_metadata={}, id='24421249-cbf7-4c0c-85ea-33f5a357b5a5'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 52, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CGho3HMwnO0SqATEHaTDkNeTTf6kZ', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--0e485660-dd1f-4dd0-b3ba-0d57aaf210ef-0', tool_calls=[{'name': 'get_user_list', 'args': {}, 'id': 'call_mUr9OP4UKpAELNm8VFwDbKVn', 'type': 'tool_call'}], usage_metadata={'input_tokens': 52, 'output_tokens': 11, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='[{\"name\": \"A\", \"age\": 10}, {\"name\": \"B\", \"age\": 20}]', name='get_user_list', id='f2896d7c-00cf-4aa9-b3b0-41e6a13b522b', tool_call_id='call_mUr9OP4UKpAELNm8VFwDbKVn'), AIMessage(content='유저 A는 10살입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 97, 'total_tokens': 107, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CGho3bIyWQVhMO5oXwnAgnFsNogmO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--16ba7818-adb4-420b-a03a-2ef191c0582c-0', usage_metadata={'input_tokens': 97, 'output_tokens': 10, 'total_tokens': 107, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "{'messages': [HumanMessage(content='A 라는 유저는 몇살이야?', additional_kwargs={}, response_metadata={}, id='24421249-cbf7-4c0c-85ea-33f5a357b5a5'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 52, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CGho3HMwnO0SqATEHaTDkNeTTf6kZ', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--0e485660-dd1f-4dd0-b3ba-0d57aaf210ef-0', tool_calls=[{'name': 'get_user_list', 'args': {}, 'id': 'call_mUr9OP4UKpAELNm8VFwDbKVn', 'type': 'tool_call'}], usage_metadata={'input_tokens': 52, 'output_tokens': 11, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='[{\"name\": \"A\", \"age\": 10}, {\"name\": \"B\", \"age\": 20}]', name='get_user_list', id='f2896d7c-00cf-4aa9-b3b0-41e6a13b522b', tool_call_id='call_mUr9OP4UKpAELNm8VFwDbKVn'), AIMessage(content='유저 A는 10살입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 97, 'total_tokens': 107, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CGho3bIyWQVhMO5oXwnAgnFsNogmO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--16ba7818-adb4-420b-a03a-2ef191c0582c-0', usage_metadata={'input_tokens': 97, 'output_tokens': 10, 'total_tokens': 107, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='내가 이전에 했던 질문 기억해?', additional_kwargs={}, response_metadata={}, id='3cdef3cf-5250-436a-988b-9556023f20aa'), AIMessage(content='네, 이전에 \"A라는 유저는 몇 살이야?\"라는 질문을 하셨고, 유저 A는 10살이라고 답변드렸습니다. 다른 질문이 있으신가요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 124, 'total_tokens': 169, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CGho5iaJsHqAqCj1oYd1tXze86sYI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--ffdf2d08-74d9-4010-8122-bc36a72a3ee9-0', usage_metadata={'input_tokens': 124, 'output_tokens': 45, 'total_tokens': 169, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "def get_user_list():\n",
    "    \"\"\"유저 정보 목록을 가져옵니다.\"\"\"\n",
    "    return [{\"name\": \"A\", \"age\": 10}, {\"name\": \"B\", \"age\": 20}]\n",
    "\n",
    "agent = create_agent(\n",
    "    # \"openai:gpt-5-nano\",\n",
    "    \"openai:gpt-4o-mini\",\n",
    "    [get_user_list],\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"A 라는 유저는 몇살이야?\"}]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n",
    "\n",
    "print(f\"{response}\")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"내가 이전에 했던 질문 기억해?\"}]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n",
    "\n",
    "print(f\"{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 운영환경 - 데이터베이스로 지원되는 체크포인터 사용\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "with PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "    agent = create_agent(\n",
    "        \"openai:gpt-5\",\n",
    "        [get_user_list],\n",
    "        checkpointer=checkpointer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf234e9",
   "metadata": {},
   "source": [
    "- 메시지 트리밍 (Message treaming)\n",
    "LLM을 호출하기 전에 첫 번째 또는 마지막 N개 메시지를 제거합니다.\n",
    "대부분의 LLM에는 지원되는 최대 컨텍스트 창(토큰으로 표시)이 있습니다.\n",
    "메시지를 잘라낼 시점을 결정하는 한 가지 방법은 메시지 기록에서 토큰을 세어 그 개수가 한계에 도달할 때마다 잘라내는 것입니다.\n",
    "LangChain을 사용하는 경우, 메시지 유틸리티를 사용해서 목록에서 유지할 토큰 수와 경계 처리에 사용할 토큰 수 를 지정할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9b298",
   "metadata": {},
   "source": [
    "\n",
    "### `trim_messages`\n",
    "\n",
    "이 함수는 지정된 \\*\\*최대 토큰 수(`max_tokens`)\\*\\*를 넘지 않도록 메시지 리스트를 자릅니다. 어떤 메시지를 남기고 어떤 메시지를 제거할지는 \\*\\*전략(`strategy`)\\*\\*과 다른 파라미터들을 통해 결정됩니다.\n",
    "\n",
    "- 주요 파라미터 분석\n",
    "`messages`\n",
    "\n",
    "  - **역할**: 잘라낼 대상이 되는 메시지 객체들의 리스트입니다.\n",
    "  - **예시**: `[HumanMessage(...), AIMessage(...), ToolMessage(...)]`\n",
    "\n",
    "`strategy`\n",
    "  - **역할**: 메시지를 자르는 방식을 결정하는 핵심 전략입니다.\n",
    "  - **`\"last\"`** (사용자 코드의 경우): 리스트의 **가장 마지막 메시지부터 역순으로** 메시지를 수집합니다. 최대 토큰 수를 넘지 않을 때까지 이 과정을 반복합니다. 이는 \"최근 N개의 메시지 유지\"와 유사하게 동작합니다.\n",
    "  - **`\"first\"`**: 리스트의 **가장 첫 번째 메시지부터 순서대로** 메시지를 수집합니다. 오래된 대화의 맥락을 유지할 때 유용할 수 있습니다.\n",
    "  - **사용자 정의 함수**: 직접 메시지 선택 로직을 구현한 함수를 전달할 수도 있습니다.\n",
    "\n",
    "`token_counter`\n",
    "  - **역할**: 각 메시지의 토큰 수를 계산하는 함수입니다.\n",
    "  - **`count_tokens_approximately`**: LangChain에서 제공하는 **근사치 계산 함수**입니다. 정확한 토큰 계산을 위해서는 모델의 실제 토크나이저(예: `tiktoken`)를 사용하는 것이 더 좋습니다.\n",
    "\n",
    "`max_tokens`\n",
    "  - **역할**: 잘라낸 후의 메시지 리스트가 포함할 수 있는 **최대 토큰 수**를 지정합니다.\n",
    "  - **동작**: 전략에 따라 메시지를 하나씩 추가하면서, 이 값을 초과하기 직전까지의 메시지만 최종 결과에 포함됩니다.\n",
    "\n",
    "`start_on`\n",
    "  - **역할**: 메시지 수집을 시작할 **메시지 타입**을 지정합니다. (선택 사항)\n",
    "  - **`\"human\"`** (사용자 코드의 경우): 메시지 리스트를 탐색할 때, `HumanMessage` 타입의 메시지를 만나면 수집을 시작합니다. 이는 대화가 항상 사용자 메시지로 시작하도록 강제하여 모델이 더 안정적으로 응답하도록 돕습니다.\n",
    "\n",
    "`end_on`\n",
    "  - **역할**: 메시지 수집을 마칠 **메시지 타입**을 지정합니다. (선택 사항)\n",
    "  - **`(\"human\", \"tool\")`** (사용자 코드의 경우): `HumanMessage` 또는 `ToolMessage` 타입의 메시지를 마지막으로 수집한 후, 프로세스를 종료합니다. 이는 잘라낸 메시지 리스트의 마지막이 AI의 답변 도중(`AIMessage`)에 끝나지 않도록 보장합니다. 즉, 완전한 \"질문-답변\" 또는 \"질문-도구사용\" 사이클을 유지하려는 의도입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9055be7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don’t know your name yet—you haven’t shared it in this chat. Tell me your name and I’ll use it, or I can suggest a dog-inspired name if you’re looking for one. Want to play a quick guessing game or just pick a nickname?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n================================== Ai Message ==================================\\n\\nYour name is Bob. You told me that earlier.\\nIf you'd like me to call you a nickname or use a different name, just say the word.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages.utils import trim_messages, count_tokens_approximately\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "def pre_model_hook(state) -> dict[str, list[BaseMessage]]:\n",
    "    \"\"\"\n",
    "    This function will be called prior to every llm call to prepare the messages for the llm.\n",
    "    \"\"\"\n",
    "\n",
    "# 1. `state[\"messages\"]` 리스트의 **가장 마지막 메시지부터** 역순으로 탐색을 시작합니다 (`strategy=\"last\"`).\n",
    "# 2. 탐색 도중 `HumanMessage`를 만나면 메시지 수집을 시작합니다 (`start_on=\"human\"`).\n",
    "# 3. 계속해서 이전 메시지들을 하나씩 수집하면서, `count_tokens_approximately`로 계산한 누적 토큰 수가 **384개**를 넘지 않는지 확인합니다 (`max_tokens=384`).\n",
    "# 4. 만약 `HumanMessage`나 `ToolMessage`를 수집한 직후에 누적 토큰이 384개를 초과하면, 해당 메시지까지만 포함하고 프로세스를 중단합니다 (`end_on`).\n",
    "# 5. 최종적으로, 잘라낸 메시지 리스트를 반환합니다. 이 리스트는 항상 `HumanMessage`로 시작하고, `HumanMessage` 또는 `ToolMessage`로 끝나며, 총 토큰 수는 384개를 넘지 않습니다.\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=100,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    return {\"llm_input_messages\": trimmed_messages}\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "agent = create_agent(\n",
    "    \"openai:gpt-5-nano\",\n",
    "    tools=[],\n",
    "    pre_model_hook=pre_model_hook,  # 모델 진입전 작동\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "\"\"\"\n",
    "================================== Ai Message ==================================\n",
    "\n",
    "Your name is Bob. You told me that earlier.\n",
    "If you'd like me to call you a nickname or use a different name, just say the word.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# 문서 기반 질의응답 MCP 서버

**문서 기반 질의응답 MCP 서버**는 AI 에이전트를 위한 **MCP(Model Context Protocol) 서버**로, AI가 특정 지식 베이스나 내부 문서를 기반으로 정확하고 근거 있는 정보를 제공할 수 있도록 돕는 지능형 어시스턴트입니다.

이 프로젝트는 Microsoft의 **Semantic Kernel**을 백엔드로 활용하여, 방대한 문서 자료를 효율적으로 관리하고 AI 에이전트의 질문에 가장 관련성 높은 정보를 찾아 제공하는 실습 프로젝트입니다.

-----

## 🎯 해결하려는 문제

범용 AI 모델은 인터넷에 공개되지 않은 최신 정보나 특정 조직의 내부 데이터에 접근할 수 없습니다. 이로 인해 AI가 제공하는 답변은 다음과 같은 한계를 가집니다.

  * **정보의 한계:** 특정 도메인의 전문 지식이나 기업의 내부 자료에 기반한 답변을 할 수 없습니다.
  * **부정확한 정보:** 오래되거나 일반적이지 않은 정보에 기반하여 잘못된 답변을 생성할 수 있습니다(환각 현상).
  * **신뢰성 부족:** 답변의 근거가 되는 원본 문서를 제시하지 못해 신뢰도가 떨어집니다.

**문서 기반 질의응답 MCP 서버**는 AI 에이전트와 로컬/비공개 문서 저장소 사이의 '다리' 역할을 수행하여, AI가 제공된 문서를 기반으로 정확하고 신뢰할 수 있는 답변을 생성하도록 돕습니다.

-----

## 🛠️ 핵심 기술 및 아키텍처

  * **MCP Server (문서 기반 질의응답 서버):** AI 에이전트에게 특정 지식 베이스와 '도구(Tools)'를 제공하는 서버입니다. **ASP.NET Core**를 기반으로 구현됩니다.
  * **Vector Database (e.g., ChromaDB, FAISS):** 사용자가 제공한 매뉴얼, 보고서, 내부 규정 등 다양한 문서를 **의미 기반으로 검색**할 수 있도록 벡터 형태로 저장하는 데이터베이스입니다.
      * **사용 데이터베이스**: 본 프로젝트에서는 **ChromaDB**를 벡터 데이터베이스로 활용합니다.
      * **ChromaDB 실행**: ChromaDB 서버는 **Docker 컨테이너**로 로컬 환경에 배포하여 사용합니다. 
        ```bash
        docker run -p 8000:8000 chromadb/chroma:0.4.14
        ```
  * **Embedding Model (임베딩 모델):** 텍스트 데이터를 벡터(수치형 배열)로 변환하여 벡터 데이터베이스에 저장하고 검색하는 데 사용됩니다.
      * **사용 모델**: 본 프로젝트에서는 **Ollama**를 통해 로컬에서 실행되는 임베딩 모델을 활용합니다. 특히 \*\*`bge-m3:latest`\*\*와 같은 모델은 다국어 지원 및 우수한 임베딩 성능을 제공합니다.
      * **Ollama 설정**:
        1.  **Ollama 설치**: [Ollama 공식 웹사이트](https://ollama.com/download)에서 로컬 환경에 설치합니다.
        2.  **모델 다운로드**: 다음 명령어를 통해 `bge-m3:latest` 모델을 Ollama에 다운로드합니다:
            ```bash
            ollama pull bge-m3
            ```
        3.  **모델 실행 확인**: `ollama list` 명령어로 다운로드된 모델 목록을 확인할 수 있습니다.
  * **.NET MCP 라이브러리 활용:** 서버 구현을 위해 **`ModelContextProtocol.AspNetCore`** NuGet 패키지를 사용합니다. 이 라이브러리는 MCP 프로토콜 통신, HTTP 엔드포인트 설정, 도구 자동 등록(`WithToolsFromAssembly`) 등 MCP 서버에 필요한 기반 인프라를 제공하여 개발자가 비즈니스 로직에 집중할 수 있도록 돕습니다.

### 동작 방식

1.  **[사용자]** AI 에이전트에게 특정 문서 기반의 질문을 합니다.
2.  **[AI 에이전트]** 사용자의 질문을 분석하고, 답변에 필요한 정보를 얻기 위해 **MCP 서버**에 검색을 요청합니다.
3.  **[MCP 서버]**
      * 사용자 질문 텍스트를 \*\*Ollama 임베딩 모델(`bge-m3:latest`)\*\*을 사용하여 벡터로 변환합니다.
      * 변환된 벡터를 이용하여 **ChromaDB**에서 의미적으로 유사한 문서 조각(chunk)을 찾아냅니다.
      * 검색된 정보를 AI가 이해하기 쉬운 형태로 가공하여 응답합니다.
4.  **[AI 에이전트]** MCP 서버로부터 받은 핵심 정보를 바탕으로, 질문에 대한 정확하고 근거 있는 최종 답변을 생성합니다.
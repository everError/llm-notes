{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.9 | packaged by conda-forge | (main, Oct 16 2025, 10:23:36) [MSC v.1944 64 bit (AMD64)]\n",
      "PyTorch version: 2.8.0+cu129\n",
      "CUDA available: True\n",
      "CUDA version: 12.9\n",
      "GPU: NVIDIA GeForce RTX 5070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import time\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLLB-200 모델 테스트\n",
    "경량화된 600M 모델부터 테스트\n",
    "\n",
    "라이센스 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLLB-600M 모델 로드\n",
    "print(\"Loading NLLB-600M model...\")\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_ko_to_en(text: str, model, tokenizer, device=\"cpu\"):\n",
    "    \"\"\"한국어를 영어로 번역\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    translated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"eng_Latn\"),\n",
    "        max_length=512,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    result = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    return result\n",
    "\n",
    "def translate_en_to_ko(text: str, model, tokenizer, device=\"cpu\"):\n",
    "    \"\"\"영어를 한국어로 번역\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    translated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"kor_Hang\"),\n",
    "        max_length=512,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    result = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 테스트 케이스 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI 에이전트 개발 관련 테스트 문장들\n",
    "test_sentences_ko = [\n",
    "    \"AI 에이전트를 개발하는 것은 매우 흥미로운 작업입니다.\",\n",
    "    \"Python으로 MCP 서버를 구축하고 있습니다.\",\n",
    "    \"머신러닝 모델의 성능을 최적화하는 방법에는 여러 가지가 있습니다.\",\n",
    "    \"한국어 프롬프트를 영어로 번역한 후 LLM에 전달합니다.\",\n",
    "    \"번역 품질을 높이기 위해 전문화된 모델을 사용합니다.\",\n",
    "    \"FastMCP를 사용하면 MCP 서버를 쉽게 만들 수 있습니다.\",\n",
    "]\n",
    "\n",
    "test_sentences_en = [\n",
    "    \"Developing an AI agent is a very interesting task.\",\n",
    "    \"I am building an MCP server with Python.\",\n",
    "    \"There are several ways to optimize machine learning model performance.\",\n",
    "    \"We translate Korean prompts into English and then pass them to the LLM.\",\n",
    "    \"We use specialized models to improve translation quality.\",\n",
    "    \"FastMCP makes it easy to create MCP servers.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "한국어 → 영어 번역 테스트 (NLLB-600M)\n",
      "================================================================================\n",
      "\n",
      "[1] 원문 (한국어):\n",
      "  AI 에이전트를 개발하는 것은 매우 흥미로운 작업입니다.\n",
      "번역 (영어):\n",
      "  Developing an AI agent is a very interesting job.\n",
      "소요 시간: 0.36초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] 원문 (한국어):\n",
      "  Python으로 MCP 서버를 구축하고 있습니다.\n",
      "번역 (영어):\n",
      "  I'm building an MCP server in Python.\n",
      "소요 시간: 0.14초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] 원문 (한국어):\n",
      "  머신러닝 모델의 성능을 최적화하는 방법에는 여러 가지가 있습니다.\n",
      "번역 (영어):\n",
      "  There are many ways to optimize the performance of machine learning models.\n",
      "소요 시간: 0.18초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4] 원문 (한국어):\n",
      "  한국어 프롬프트를 영어로 번역한 후 LLM에 전달합니다.\n",
      "번역 (영어):\n",
      "  Korean Language Prompt is translated into English and then transferred to LLM.\n",
      "소요 시간: 0.20초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[5] 원문 (한국어):\n",
      "  번역 품질을 높이기 위해 전문화된 모델을 사용합니다.\n",
      "번역 (영어):\n",
      "  We use specialized models to improve the quality of translation.\n",
      "소요 시간: 0.17초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[6] 원문 (한국어):\n",
      "  FastMCP를 사용하면 MCP 서버를 쉽게 만들 수 있습니다.\n",
      "번역 (영어):\n",
      "  Using FastMCP you can easily create MCP servers.\n",
      "소요 시간: 0.17초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "평균 번역 시간: 0.20초\n"
     ]
    }
   ],
   "source": [
    "# 한국어 → 영어 번역 테스트\n",
    "print(\"=\" * 80)\n",
    "print(\"한국어 → 영어 번역 테스트 (NLLB-600M)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_time = 0\n",
    "for i, ko_text in enumerate(test_sentences_ko, 1):\n",
    "    start_time = time.time()\n",
    "    en_text = translate_ko_to_en(ko_text, model, tokenizer, device)\n",
    "    elapsed = time.time() - start_time\n",
    "    total_time += elapsed\n",
    "    \n",
    "    print(f\"\\n[{i}] 원문 (한국어):\")\n",
    "    print(f\"  {ko_text}\")\n",
    "    print(f\"번역 (영어):\")\n",
    "    print(f\"  {en_text}\")\n",
    "    print(f\"소요 시간: {elapsed:.2f}초\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n평균 번역 시간: {total_time / len(test_sentences_ko):.2f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "영어 → 한국어 번역 테스트 (NLLB-600M)\n",
      "================================================================================\n",
      "\n",
      "[1] 원문 (영어):\n",
      "  Developing an AI agent is a very interesting task.\n",
      "번역 (한국어):\n",
      "  인공지능 에이전트를 개발하는 것은 매우 흥미로운 작업입니다.\n",
      "소요 시간: 0.32초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] 원문 (영어):\n",
      "  I am building an MCP server with Python.\n",
      "번역 (한국어):\n",
      "  파이썬으로 MCP 서버를 만들고 있습니다.\n",
      "소요 시간: 0.14초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] 원문 (영어):\n",
      "  There are several ways to optimize machine learning model performance.\n",
      "번역 (한국어):\n",
      "  머신러닝 모델의 성능을 최적화하는 몇 가지 방법이 있습니다.\n",
      "소요 시간: 0.21초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4] 원문 (영어):\n",
      "  We translate Korean prompts into English and then pass them to the LLM.\n",
      "번역 (한국어):\n",
      "  우리는 한국어 문서를 영어로 번역하고 LLM에 전달합니다.\n",
      "소요 시간: 0.18초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[5] 원문 (영어):\n",
      "  We use specialized models to improve translation quality.\n",
      "번역 (한국어):\n",
      "  우리는 전문적인 모델을 사용하여 번역의 질을 향상시킵니다.\n",
      "소요 시간: 0.16초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[6] 원문 (영어):\n",
      "  FastMCP makes it easy to create MCP servers.\n",
      "번역 (한국어):\n",
      "  FastMCP는 MCP 서버를 쉽게 만들 수 있습니다.\n",
      "소요 시간: 0.18초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "평균 번역 시간: 0.20초\n"
     ]
    }
   ],
   "source": [
    "# 영어 → 한국어 번역 테스트\n",
    "print(\"=\" * 80)\n",
    "print(\"영어 → 한국어 번역 테스트 (NLLB-600M)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_time = 0\n",
    "for i, en_text in enumerate(test_sentences_en, 1):\n",
    "    start_time = time.time()\n",
    "    ko_text = translate_en_to_ko(en_text, model, tokenizer, device)\n",
    "    elapsed = time.time() - start_time\n",
    "    total_time += elapsed\n",
    "    \n",
    "    print(f\"\\n[{i}] 원문 (영어):\")\n",
    "    print(f\"  {en_text}\")\n",
    "    print(f\"번역 (한국어):\")\n",
    "    print(f\"  {ko_text}\")\n",
    "    print(f\"소요 시간: {elapsed:.2f}초\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n평균 번역 시간: {total_time / len(test_sentences_en):.2f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences_ko = [\n",
    "    \"공정 목록을 알려주세요\",\n",
    "    \"2025년 8월1일부터 9월30일까지 생산계획을 알려주세요.\",\n",
    "    \"작업지시 'PD202509010017'의 총 생산실적 수량을 알려주세요.\",\n",
    "    \"'5060150100PU1' 품목의 재고가 얼마나 있는지 알려주세요.\",\n",
    "    \"오늘 출고(출하)예정인 품목과 수량 정보를 알려주세요.\",\n",
    "    \"'PD202504170013' 작업지시에 투입해야할 투입자재 정보를 품목코드, 품목명, 유형, BOM 사용수량, 총 투입수량 형태로 알려주세요.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 왕복 번역 테스트 (품질 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "왕복 번역 테스트 (한국어 → 영어 → 한국어)\n",
      "================================================================================\n",
      "\n",
      "[1] 원본 한국어:\n",
      "  공정 목록을 알려주세요\n",
      "→ 영어 번역:\n",
      "  Please tell me the list.\n",
      "→ 다시 한국어로:\n",
      "  리스트를 알려주세요.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] 원본 한국어:\n",
      "  2025년 8월1일부터 9월30일까지 생산계획을 알려주세요.\n",
      "→ 영어 번역:\n",
      "  Please indicate the production plan from 1 August 2025 to 30 September 2025.\n",
      "→ 다시 한국어로:\n",
      "  2025년 8월 1일부터 2025년 9월 30일까지의 생산 계획을 알려주세요.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] 원본 한국어:\n",
      "  작업지시 'PD202509010017'의 총 생산실적 수량을 알려주세요.\n",
      "→ 영어 번역:\n",
      "  Please indicate the total actual production volume of 'PD202509010017'.\n",
      "→ 다시 한국어로:\n",
      "  \"PD202509010017\"의 전체 실제 생산량을 표시하십시오.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4] 원본 한국어:\n",
      "  '5060150100PU1' 품목의 재고가 얼마나 있는지 알려주세요.\n",
      "→ 영어 번역:\n",
      "  Please tell me how much is the stock of the item '5060150100PU1'.\n",
      "→ 다시 한국어로:\n",
      "  \"5060150100PU1\" 항목의 주가가 얼마나 되는지 알려주세요.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[5] 원본 한국어:\n",
      "  오늘 출고(출하)예정인 품목과 수량 정보를 알려주세요.\n",
      "→ 영어 번역:\n",
      "  Let me know if you have any information.\n",
      "→ 다시 한국어로:\n",
      "  혹시 정보가 있다면 알려주세요.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[6] 원본 한국어:\n",
      "  'PD202504170013' 작업지시에 투입해야할 투입자재 정보를 품목코드, 품목명, 유형, BOM 사용수량, 총 투입수량 형태로 알려주세요.\n",
      "→ 영어 번역:\n",
      "  Please indicate the information to be entered in the 'PD202504170013' worksheet in the form of item code, item name, type, BOM usage quantity, 총 투입수량.\n",
      "→ 다시 한국어로:\n",
      "  \"PD202504170013\" 워크시트에 항목 코드, 항목 이름, 유형, BOM 사용량, 총 투입수량 형태로 입력해야 할 정보를 표시하십시오.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"왕복 번역 테스트 (한국어 → 영어 → 한국어)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, original_ko in enumerate(test_sentences_ko[:6], 1):\n",
    "    # 한국어 → 영어\n",
    "    en_translated = translate_ko_to_en(original_ko, model, tokenizer, device)\n",
    "    \n",
    "    # 영어 → 한국어 (다시 번역)\n",
    "    ko_back = translate_en_to_ko(en_translated, model, tokenizer, device)\n",
    "    \n",
    "    print(f\"\\n[{i}] 원본 한국어:\")\n",
    "    print(f\"  {original_ko}\")\n",
    "    print(f\"→ 영어 번역:\")\n",
    "    print(f\"  {en_translated}\")\n",
    "    print(f\"→ 다시 한국어로:\")\n",
    "    print(f\"  {ko_back}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 대화형 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 입력 테스트\n",
    "your_text = \"여기에 번역하고 싶은 한국어 문장을 입력하세요.\"\n",
    "result = translate_ko_to_en(your_text, model, tokenizer, device)\n",
    "print(f\"원문: {your_text}\")\n",
    "print(f\"번역: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 입력 테스트\n",
    "your_english = \"Enter your English sentence here to translate to Korean.\"\n",
    "result = translate_en_to_ko(your_english, model, tokenizer, device)\n",
    "print(f\"Original: {your_english}\")\n",
    "print(f\"Translation: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델: facebook/nllb-200-distilled-600M\n",
      "총 파라미터 수: 615,073,792\n",
      "모델 크기: 615.1M parameters\n",
      "\n",
      "GPU 메모리 사용량: 2.48 GB\n",
      "GPU 메모리 예약량: 2.55 GB\n"
     ]
    }
   ],
   "source": [
    "# 모델 크기 및 파라미터 수\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(f\"모델: {model_name}\")\n",
    "print(f\"총 파라미터 수: {total_params:,}\")\n",
    "print(f\"모델 크기: {total_params / 1e6:.1f}M parameters\")\n",
    "\n",
    "# 메모리 사용량\n",
    "if device == \"cuda\":\n",
    "    print(f\"\\nGPU 메모리 사용량: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU 메모리 예약량: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (선택) Tower 모델 테스트\n",
    "번역 전문 모델인 Tower와 비교해보고 싶다면 아래 셀을 실행하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tower 모델 로드 (주석 해제하여 사용)\n",
    "tower_model_name = \"Unbabel/TowerInstruct-7B-v0.2\"\n",
    "print(f\"Loading {tower_model_name}...\")\n",
    "tower_tokenizer = AutoTokenizer.from_pretrained(tower_model_name)\n",
    "tower_model = AutoModelForSeq2SeqLM.from_pretrained(tower_model_name)\n",
    "tower_model = tower_model.to(device)\n",
    "print(\"Tower model loaded!\")\n",
    "\n",
    "# Tower 모델로 번역 테스트\n",
    "test_text = \"AI 에이전트 개발은 흥미롭습니다.\"\n",
    "result = translate_ko_to_en(test_text, tower_model, tower_tokenizer, device)\n",
    "print(f\"원문: {test_text}\")\n",
    "print(f\"Tower 번역: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
